name: Production Env Tests

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]

jobs:
  run_tests:
    runs-on: ubuntu-latest
    env:
      CLUSTER_NAME: exaflow
      CLUSTER_PORT: "5000"
      CLUSTER_NODEPORT: "30000"

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: 1.8.2
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v3
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ hashFiles('poetry.lock') }}

      - name: Install dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-interaction --no-root

      - name: Build data paths for workers
        run: |
          python worker_data_path_builder.py \
            --local-workers localworker3 localworker2 localworker1 \
            --global-worker globalworker \
            --test-data-folder tests/test_data

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build WORKER service docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: exaflow/worker/Dockerfile
          push: false
          load: true
          tags: madgik/exaflow_worker:dev
          cache-from: type=gha,scope=worker
          cache-to: type=gha,scope=worker,mode=max

      - name: Build CONTROLLER service docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: exaflow/controller/Dockerfile
          push: false
          load: true
          tags: madgik/exaflow_controller:dev
          cache-from: type=gha,scope=controller
          cache-to: type=gha,scope=controller,mode=max

      - name: Build AGGREGATION SERVER docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: aggregation_server/Dockerfile
          push: false
          load: true
          tags: madgik/exaflow_aggregation_server:dev
          cache-from: type=gha,scope=aggregation-server
          cache-to: type=gha,scope=aggregation-server,mode=max

      - name: Install k3d
        run: |
          curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | sudo bash

      - name: Create k3d Cluster
        run: |
          k3d cluster create "$CLUSTER_NAME" \
            --servers 1 \
            --agents 3 \
            --api-port 6443 \
            --port "${CLUSTER_PORT}:${CLUSTER_NODEPORT}@loadbalancer" \
            --k3s-arg '--disable=traefik@server:0' \
            --volume "$PWD/tests/prod_env_tests/deployment_configs/kind_configuration/master/hostname:/etc/hostname@server:0" \
            --volume "$PWD/tests/test_data/.data_paths/globalworker:/opt/exaflow/globalworker/csvs@server:0" \
            --volume "$PWD/tests/prod_env_tests/deployment_configs/kind_configuration/worker1/hostname:/etc/hostname@agent:0" \
            --volume "$PWD/tests/test_data/.data_paths/localworker1:/opt/exaflow/localworker/csvs@agent:0" \
            --volume "$PWD/tests/prod_env_tests/deployment_configs/kind_configuration/worker2/hostname:/etc/hostname@agent:1" \
            --volume "$PWD/tests/test_data/.data_paths/localworker2:/opt/exaflow/localworker/csvs@agent:1" \
            --volume "$PWD/tests/prod_env_tests/deployment_configs/kind_configuration/worker3/hostname:/etc/hostname@agent:2" \
            --volume "$PWD/tests/test_data/.data_paths/localworker3:/opt/exaflow/localworker/csvs@agent:2"

      - name: Label Nodes
        run: |
          set -euo pipefail
          server_node=$(kubectl get nodes -o name | grep server | head -n1 | cut -d/ -f2)
          worker_nodes=$(kubectl get nodes -o name | grep agent | cut -d/ -f2)
          echo "Server node: $server_node"
          echo "Worker nodes: $worker_nodes"
          kubectl label node "$server_node" master=true --overwrite
          for n in $worker_nodes; do
            kubectl label node "$n" worker=true --overwrite
          done

      - name: Import images into k3d
        run: |
          k3d image import \
            madgik/exaflow_worker:dev \
            madgik/exaflow_controller:dev \
            madgik/exaflow_aggregation_server:dev \
            --cluster "$CLUSTER_NAME"

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 3.9.1
        id: install

      - name: Get container disk space
        run: df -h

      - name: Copy prod_env_tests values.yaml
        run: cp -r tests/prod_env_tests/deployment_configs/kubernetes_values.yaml kubernetes/values.yaml

      - name: Print Helm Templates
        run: helm template kubernetes/

      - name: Deploy Helm
        run: helm install exaflow kubernetes/ --debug

      - name: Wait for pods to get healthy
        run: |
          timeout 300 bash -c '
          set -euo pipefail

          while true; do
            # Get pods and check if all are ready (1/1, 2/2, 3/3, 4/4)
            if kubectl get pods --no-headers | awk '\''{if ($2 != "1/1" && $2 != "2/2" && $2 != "3/3" && $2 != "4/4") exit 1;}'\''; then
              echo "All pods are ready!"
              break
            else
              echo "Some pods are not ready yet:"
              kubectl get pods -o wide

              # Find clearly "bad" pods (crashing / failing)
              crashing_pods=$(kubectl get pods --no-headers \
                | awk '\''$3 ~ /CrashLoopBackOff|Error|ImagePullBackOff|CreateContainerError/ {print $1}'\'')

              if [ -n "$crashing_pods" ]; then
                echo "Detected crashing pods: $crashing_pods"

                for p in $crashing_pods; do
                  echo "===== DESCRIBE POD $p ====="
                  kubectl describe pod "$p" || true

                  echo "===== LOGS POD $p (all containers) ====="
                  kubectl logs "$p" --all-containers --tail=200 || true

                  echo "===== PREVIOUS LOGS POD $p (if any) ====="
                  kubectl logs "$p" --all-containers --previous --tail=200 || true
                done

                echo "Failing because at least one pod is crashing."
                exit 1
              fi

              sleep 5
            fi
          done
          '


      - name: Controller logs
        run: kubectl logs -l app=exaflow-controller --tail -1

      - name: Globalnode logs
        run: kubectl logs -l nodeType=globalworker -c worker --tail -1

      - name: Localnode logs
        run: kubectl logs -l nodeType=localworker -c worker --tail -1

      - name: Controller logs (post run)
        uses: webiny/action-post-run@3.0.0
        with:
          run: kubectl logs -l app=exaflow-controller --tail -1

      - name: Globalnode logs (post run)
        uses: webiny/action-post-run@3.0.0
        with:
          run: kubectl logs -l nodeType=globalworker -c worker --tail -1

      - name: Localnode logs (post run)
        uses: webiny/action-post-run@3.0.0
        with:
          run: kubectl logs -l nodeType=localworker -c worker --tail -1

      - name: Wait before WLA update
        run: sleep 5

      - name: Run Worker Landscape Aggregator update
        run: curl -X POST "http://127.0.0.1:${CLUSTER_PORT}/wla"

      - name: Run Healthcheck
        run: curl "http://127.0.0.1:${CLUSTER_PORT}/healthcheck"

      - name: Run production env tests
        run: poetry run pytest tests/prod_env_tests --verbosity=4
